{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be435dfa9f025792",
   "metadata": {},
   "source": [
    "# Simple Experiments: A Tutorial\n",
    "\n",
    "This notebook serves as an introductory tour to this code base. We will run a simple experiment with simplified data in order to\n",
    "introduce relevant functionalities and understand what each part is doing.\n",
    "\n",
    "Real \"production\" grade experiments would not be conducted through a notebook like this one. For production, use\n",
    "the main entry points and configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079bb046-388c-4935-9e86-46154d803895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472778f454e0515c",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "\n",
    "For the purpose of this tutorial, we will generate an on-the-fly simplified dataset. We will draw samples from a simple isotropic Gaussian\n",
    "distribution centered around the equilibrium coordinates of crystalline silicon. This data is composed of the relative coordinates of 8 atoms in 3D.\n",
    "This is not representative of a real dataset: \"real\" silicon atoms would be displaced according to thermalized phonons, not a simple isotropic Normal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945d1e7b49e6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports and defining global variables\n",
    "import numpy as np\n",
    "from diffusion_for_multi_scale_molecular_dynamics.loss.loss_parameters import MSELossParameters\n",
    "from diffusion_for_multi_scale_molecular_dynamics.noise_schedulers.noise_parameters import NoiseParameters\n",
    "from diffusion_for_multi_scale_molecular_dynamics.utils.reference_configurations import get_silicon_supercell\n",
    "\n",
    "silicon_equilibrium_relative_coordinates = get_silicon_supercell(supercell_factor=1).astype(np.float32)\n",
    "equilibrium_relative_coordinates = list(list(frac) for frac in silicon_equilibrium_relative_coordinates)\n",
    "number_of_atoms = 8\n",
    "spatial_dimension = 3\n",
    "\n",
    "# The dataset will be a sample from an isotropic Gaussian centered on the equilibrium relative coordinates.\n",
    "sigma_d = 0.05\n",
    "\n",
    "# Let's choose convenient dataset sizes so that the code executes quickly.\n",
    "train_dataset_size = 2_048\n",
    "valid_dataset_size = 512\n",
    "\n",
    "# Diffusion models rely on introducing \"noise\" which scrambles the original data. The goal of the diffusion model\n",
    "# is to start from a completely scrambled sample and to \"denoise\" it back to something similar to the data it\n",
    "# was trained on.\n",
    "\n",
    "# We must define the parameters of this noising process.\n",
    "noise_parameters = NoiseParameters(total_time_steps=200,\n",
    "                                   schedule_type=\"exponential\",\n",
    "                                   sigma_min=0.005,\n",
    "                                   sigma_max=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a7dfdfba1706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_for_multi_scale_molecular_dynamics.data.diffusion.gaussian_data_module import \\\n",
    "    GaussianDataModuleParameters, GaussianDataModule\n",
    "\n",
    "\n",
    "gaussian_datamodule_parameters = GaussianDataModuleParameters(noise_parameters=noise_parameters,\n",
    "                                                              elements=[\"Si\"],\n",
    "                                                              use_optimal_transport=False,\n",
    "                                                              random_seed=42,\n",
    "                                                              number_of_atoms=number_of_atoms,\n",
    "                                                              sigma_d=sigma_d,\n",
    "                                                              equilibrium_relative_coordinates=equilibrium_relative_coordinates,\n",
    "                                                              train_dataset_size=train_dataset_size,\n",
    "                                                              valid_dataset_size=valid_dataset_size,\n",
    "                                                              batch_size=512,\n",
    "                                                              num_workers=8,\n",
    "                                                              max_atom=number_of_atoms,\n",
    "                                                              spatial_dimension=spatial_dimension)\n",
    "\n",
    "data_module = GaussianDataModule(gaussian_datamodule_parameters)\n",
    "\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff105a3-0ef7-4610-84be-c2b92b6e32ea",
   "metadata": {},
   "source": [
    "# The AXL Diffusion Lightning Model\n",
    "\n",
    "The main \"model\" in charge of predicting how to denoise a sample is an \"AXL network\". The Pytorch Lightning-derived AXLDiffusionLightningMoldel is the class in charge of managing all the needed inputs and outputs to train an AXL network.\n",
    "\n",
    "Creating this object requires a fair bit of configuration. We'll build these configurations now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc3fb8-7d27-48bd-979f-e5f4fad64e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_for_multi_scale_molecular_dynamics.namespace import AXL\n",
    "from diffusion_for_multi_scale_molecular_dynamics.loss.loss_parameters import AtomTypeLossParameters\n",
    "from diffusion_for_multi_scale_molecular_dynamics.models.scheduler import ReduceLROnPlateauSchedulerParameters\n",
    "from diffusion_for_multi_scale_molecular_dynamics.models.optimizer import OptimizerParameters\n",
    "from diffusion_for_multi_scale_molecular_dynamics.models.score_networks.mlp_score_network import \\\n",
    "    MLPScoreNetworkParameters\n",
    "from diffusion_for_multi_scale_molecular_dynamics.models.axl_diffusion_lightning_model import AXLDiffusionParameters, \\\n",
    "    AXLDiffusionLightningModel\n",
    "\n",
    "\n",
    "# Score Network parameters\n",
    "# -------------------------\n",
    "# This is the machine learning model that will be taught how to denoise samples. It is the main \"engine\" on which\n",
    "# everything else rests. There are many possible choices available: here we pick a simple MLP for convenience. Note\n",
    "# that the MLP is not equivariant; this score network is really just for sanity checking and simplified experiments.\n",
    "score_network_parameters = MLPScoreNetworkParameters(number_of_atoms=number_of_atoms,\n",
    "                                                     spatial_dimension=spatial_dimension,\n",
    "                                                     num_atom_types=1,\n",
    "                                                     n_hidden_dimensions=3,\n",
    "                                                     hidden_dimensions_size=128,\n",
    "                                                     noise_embedding_dimensions_size=8,\n",
    "                                                     relative_coordinates_embedding_dimensions_size=64,\n",
    "                                                     time_embedding_dimensions_size=8,\n",
    "                                                     atom_type_embedding_dimensions_size=8,\n",
    "                                                     lattice_parameters_embedding_dimensions_size=8)\n",
    "\n",
    "\n",
    "# Loss parameters\n",
    "#-----------------\n",
    "# We must define the various lambda weights for the loss. Since we'll only consider diffusion over relative coordinates\n",
    "# we can simply turn off the loss for atom types and lattice.\n",
    "loss_parameters = AXL(A=AtomTypeLossParameters(lambda_weight=0.0),\n",
    "                      X=MSELossParameters(lambda_weight=1.0),\n",
    "                      L=MSELossParameters(lambda_weight=0.0))\n",
    "\n",
    "\n",
    "# Optimizer and Scheduler parameters\n",
    "#-----------------------------------\n",
    "# We must define some standard algorithmic hyper parameters that control the training process.\n",
    "optimizer_parameters = OptimizerParameters(name=\"adamw\",\n",
    "                                           learning_rate=0.001,\n",
    "                                           weight_decay=1.0e-8)\n",
    "\n",
    "scheduler_parameters = ReduceLROnPlateauSchedulerParameters(factor=0.5, patience=10)\n",
    "\n",
    "\n",
    "# kmax_target_score is a convergence parameter for the Ewald-like sum of the perturbation kernel for coordinates. This\n",
    "# is used to compute the targets for the score network operating on relative coordinates.\n",
    "kmax_target_score = 4\n",
    "\n",
    "# AXL Diffusion Parameters:  we combine the various configurations above into a single input for the Model.\n",
    "diffusion_parameters = AXLDiffusionParameters(score_network_parameters=score_network_parameters,\n",
    "                                              loss_parameters=loss_parameters,\n",
    "                                              optimizer_parameters=optimizer_parameters,\n",
    "                                              scheduler_parameters=scheduler_parameters,\n",
    "                                              kmax_target_score=kmax_target_score)\n",
    "\n",
    "# Create the Pytorch-Lightning \"model\", the class that a Trainer object operates on.\n",
    "model = AXLDiffusionLightningModel(diffusion_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb368278096128ff",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We can finally train the model. We leverage Pytorch-Lightning to train the model: this gives us easy access to convenient metrology tools to keep\n",
    "track of training as it progesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df4d70e420b045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_for_multi_scale_molecular_dynamics import TOP_DIR\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from diffusion_for_multi_scale_molecular_dynamics.callbacks.callback_loader import create_all_callbacks\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "# Logger\n",
    "#-------\n",
    "# A \"logger\" keeps track of various artifacts during training (the loss, for instance) and lets us visualize what is going on.\n",
    "# Many loggers such as Wandb or Comet will connect directly to the cloud so progress can be monitored on a different machine from where\n",
    "# training is done: this is useful when a job runs on a cluster. Here, we'll use Tensorboard, a completely local solution. Using notebook magics,\n",
    "# we will embed the Tensorboard ui within this notebook.\n",
    "\n",
    "# We must define a name for the experiment and a local folder where artifacts will be written.\n",
    "\n",
    "experiment_name = \"tutorial_mlp\"\n",
    "output_path = TOP_DIR / \"experiments\" / \"tutorials\" / \"output\" / experiment_name\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_directory = str(output_path)\n",
    "\n",
    "tensorboard_logger = TensorBoardLogger(save_dir=output_directory,\n",
    "                                       default_hp_metric=False,\n",
    "                                       name=experiment_name,\n",
    "                                       version=0)\n",
    "\n",
    "# Callbacks\n",
    "# ---------\n",
    "# A \"callback\" is an object with internal methods that we can pass to Pytorch-Lightning (PL); PL will \"call back\"\n",
    "# these various methods at specified points during execution, allowing us to do some metrology on the ongoing training.\n",
    "\n",
    "# Loss monitoring will plot the loss at every epochs in bins over the noising time axis.\n",
    "loss_monitoring_parameters = dict(number_of_bins=50,\n",
    "                                  sample_every_n_epochs=1,\n",
    "                                  spatial_dimension=spatial_dimension)\n",
    "\n",
    "early_stopping_parameters = dict(metric=\"validation_epoch_loss\",\n",
    "                                 mode=\"min\",\n",
    "                                 patience=20)\n",
    "\n",
    "callback_parameters = dict(loss_monitoring=loss_monitoring_parameters, early_stopping=early_stopping_parameters)\n",
    "\n",
    "callbacks_dict = create_all_callbacks(callback_parameters,\n",
    "                                      output_directory=output_directory,\n",
    "                                      verbose=True)\n",
    "\n",
    "callbacks = list(callbacks_dict.values())\n",
    "\n",
    "trainer = Trainer(callbacks=callbacks,\n",
    "                  max_epochs=300,\n",
    "                  log_every_n_steps=1,\n",
    "                  fast_dev_run=False,\n",
    "                  logger=tensorboard_logger,\n",
    "                  enable_progress_bar=True)\n",
    "\n",
    "# We will monitor progess using Tensorboard, which will be embedded in this notebook.\n",
    "%tensorboard --logdir output --samples_per_plugin images=99999\n",
    "\n",
    "# Training can take a few minutes...\n",
    "trainer.fit(model, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47482efdc56763cc",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "Now that the model is trained, we can draw new samples with it. In order to do so, we create a \"generator\" which is responsible for\n",
    "creating new samples by using the axl_network to denoise random starting points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a955914cfc1d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_for_multi_scale_molecular_dynamics.namespace import AXL_COMPOSITION\n",
    "from diffusion_for_multi_scale_molecular_dynamics.sampling.diffusion_sampling import create_batch_of_samples\n",
    "import torch\n",
    "from diffusion_for_multi_scale_molecular_dynamics.generators.predictor_corrector_axl_generator import \\\n",
    "    PredictorCorrectorSamplingParameters\n",
    "from diffusion_for_multi_scale_molecular_dynamics.generators.langevin_generator import LangevinGenerator\n",
    "\n",
    "# Sampling parameters\n",
    "cell_dimensions = [5.43, 5.43, 5.43] # Si cell cubic cell side lengths.\n",
    "\n",
    "sampling_parameters = PredictorCorrectorSamplingParameters(number_of_samples=1_024,\n",
    "                                                           spatial_dimension=spatial_dimension,\n",
    "                                                           number_of_corrector_steps=1,\n",
    "                                                           num_atom_types=1,\n",
    "                                                           number_of_atoms=number_of_atoms,\n",
    "                                                           use_fixed_lattice_parameters=True,\n",
    "                                                           cell_dimensions=cell_dimensions)\n",
    "\n",
    "\n",
    "generator = LangevinGenerator(noise_parameters=noise_parameters,\n",
    "                              sampling_parameters=sampling_parameters,\n",
    "                              axl_network=model.axl_network)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples_batch = create_batch_of_samples(generator=generator,\n",
    "                                            sampling_parameters=sampling_parameters,\n",
    "                                            device=model.device)\n",
    "\n",
    "# We can extract the sampled relative coordinates\n",
    "sampled_relative_coordinates = samples_batch[AXL_COMPOSITION].X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95b1be300e0bde",
   "metadata": {},
   "source": [
    "# Quantifying Results\n",
    "\n",
    "We can can gauge the quality of the samples by computing their total displacements from the equilibrium relative coordinates, and\n",
    "seeing how the distribution of these displacements compare with the validation dataset. We'll define a simple function to extract\n",
    "the total distances, and then we'll plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412d22c616b06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "\n",
    "def compute_total_distance(relative_coordinates: torch.Tensor, reference_relative_coordinates: torch.Tensor) -> float:\n",
    "    \"\"\" Compute total distance.\n",
    "\n",
    "    This method computes the \"total distance\" between two configurations, accounting for periodicity,\n",
    "    by comparing coordinates in order.\n",
    "\n",
    "    Args:\n",
    "        relative_coordinates: the relative coordinates of a configuration. Dimension [number_of_atoms, spatial_dimension]\n",
    "        reference_relative_coordinates: the reference relative coordinates of a configuration.\n",
    "            Dimension [number_of_atoms, spatial_dimension]\n",
    "\n",
    "    Returns:\n",
    "        Total distance: the total distance between the relative coordinates and the reference, in reduced units.\n",
    "    \"\"\"\n",
    "    raw_displacements = relative_coordinates - reference_relative_coordinates\n",
    "    augmented_displacements = [raw_displacements - 1.0, raw_displacements, raw_displacements + 1.0]\n",
    "\n",
    "    squared_displacements = einops.rearrange(augmented_displacements, \"c n d -> (n d) c\")**2\n",
    "\n",
    "    total_displacement = torch.sqrt(squared_displacements.min(dim=1).values.sum())\n",
    "    return total_displacement.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdf0eb3ca2082af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the distances for samples and validation datasets\n",
    "\n",
    "reference_relative_coordinates = torch.from_numpy(silicon_equilibrium_relative_coordinates)\n",
    "\n",
    "sampled_distances = [compute_total_distance(relative_coordinates, reference_relative_coordinates)\n",
    "                        for relative_coordinates in sampled_relative_coordinates]\n",
    "\n",
    "\n",
    "validation_distances = []\n",
    "for row in data_module.valid_dataset:\n",
    "    relative_coordinates = row['relative_coordinates']\n",
    "    distance = compute_total_distance(relative_coordinates, reference_relative_coordinates)\n",
    "    validation_distances.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9ec27-96e9-4503-aad0-3e0ea5cc03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_for_multi_scale_molecular_dynamics.analysis import PLEASANT_FIG_SIZE\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=PLEASANT_FIG_SIZE)\n",
    "fig.suptitle(f\"Sampling Displacement Distributions\")\n",
    "common_params = dict(density=True, bins=100, histtype=\"stepfilled\", alpha=0.25)\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.hist(sampled_distances, **common_params, label=f\"Samples\", color=\"red\")\n",
    "ax1.hist(validation_distances, **common_params, label=f\"Validation Data\", color=\"green\")\n",
    "\n",
    "ax1.set_xlabel(\"Total Displacement (Unitless)\")\n",
    "ax1.set_ylabel(\"Density\")\n",
    "ax1.legend(loc=\"upper right\", fancybox=True, shadow=True, ncol=1, fontsize=12)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
